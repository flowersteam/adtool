import codecs
import json
import os
import pickle
import tempfile
from typing import Dict, List, Tuple

import auto_disc.utils.leaf.locators.LinearBase as LinearBase
import requests
from auto_disc.utils.leaf.Leaf import LeafUID
from auto_disc.utils.leaf.locators.LinearBase import FileLinearLocator
from auto_disc.utils.leaf.locators.Locator import Locator
import sys

def _initialize_checkpoint(entrypoint: str, dict: Dict = {}) -> str:
    """
    Inserts a Mongo document and retrieves the unique key generated
    """
    response = requests.post(entrypoint, json=dict).json()
    return response["ID"]


def _query_uid(resource_uri: str, uid: LeafUID) -> List[Dict]:
    uid_filter = {"uid": uid}
    uid_filter = _format_filter_from_dict(uid_filter)
    response = requests.get(resource_uri + "?filter=" + uid_filter).json()
    return response


def _format_filter_from_dict(filter_dict: Dict) -> str:
    """
    Creates appropriate filter str from parsing a dict
    """
    filters = []
    for key in filter_dict.keys():
        if filter_dict[key] == None:
            pass
        elif isinstance(filter_dict[key], list):
            filters.append(
                '{{"{}":{{"$in":[{}]}} }}'.format(key), ", ".join(filter_dict[key])
            )
        else:
            if isinstance(filter_dict[key], str):
                filters.append('{{"{}":"{}" }}'.format(key, filter_dict[key]))
            else:
                filters.append('{{"{}":{} }}'.format(key, filter_dict[key]))

    # join array elements of filters to string filter
    if len(filters) > 1:
        filter = '{"$and":['
        for f in filters:
            filter += f + ","
        filter = filter[:-1]
        filter += "]}"
    else:
        filter = filters[0]
    return filter


class ExpeDBLocator(Locator):
    """
    Locator which saves modules Ã  la Git, to ExpeDB,
    with the entrypoint URL (i.e., DB) specified by resource_uri
    """

    def __init__(self, resource_uri: str = ""):
        # strip trailing /
        if len(resource_uri) > 0 and resource_uri[-1] == "/":
            resource_uri = resource_uri[:-1]
        self.resource_uri = resource_uri

    def store(self, bin: bytes, *args, **kwargs) -> "LeafUID":
        uid = self.hash(bin)
        # encode in base64 so MongoDB doesn't escape anything
        bin = codecs.encode(bin, encoding="base64")

        mongo_id = self._retrieve_mongo_id(uid)

        # update the uid field
        # TODO: refactor me
        entrypoint_url = self.resource_uri + "/" + mongo_id
        doc_update_dict = {"uid": uid}
        requests.post(entrypoint_url, json=doc_update_dict)

        # attach files
        entrypoint_url = self.resource_uri + "/" + mongo_id + "/files"
        file_dict = {"metadata": bin}
        requests.post(entrypoint_url, files=file_dict)

        return uid

    def retrieve(self, uid: "LeafUID", *args, **kwargs) -> bytes:
        # extra info past : is for other locators
        uid = uid.split(":")[0]

        mongo_id = self._retrieve_mongo_id(uid)
        response_bin = requests.get(
            self.resource_uri + "/" + mongo_id + "/metadata"
        ).content
        bin = codecs.decode(response_bin, encoding="base64")
        return bin

    def _retrieve_mongo_id(self, uid: LeafUID) -> str:
        response = _query_uid(self.resource_uri, uid)
        print('_retrieve_mongo_id response', response)
        if len(response) == 0:
            mongo_id = _initialize_checkpoint(self.resource_uri)
        elif len(response) == 1:
            mongo_id = response[0]["_id"]
        else:
            raise Exception("ExpeDB is corrupted with duplicate checkpoints.")

        return mongo_id


class ExpeDBLinearLocator(Locator):
    """
    Locator which stores branching, linear data to ExpeDB,
    with the entrypoint URL (i.e., DB) specified by resource_uri
    """

    def __init__(self, resource_uri: str = ""):
        # strip trailing /
        if len(resource_uri) > 0 and resource_uri[-1] == "/":
            resource_uri = resource_uri[:-1]
        self.resource_uri = resource_uri
        self.parent_id = -1

    def store(self, bin: bytes, parent_id: int = -1) -> "LeafUID":
        # default setting if not set at function call
        if (self.parent_id != -1) and (parent_id == -1):
            parent_id = self.parent_id

        # init cachedir
        cache_dir = self._init_cache_dir()

        # parse formatted binary
        lineardb_name, data_bin = FileLinearLocator.parse_bin(bin)

        # store metadata binary
        mongo_id = self._retrieve_tree_and_store_metadata(
            cache_dir, lineardb_name, data_bin
        )

        # store data binary
        row_id = LinearBase.store_data(cache_dir, data_bin, parent_id)
        # update parent_id in instance
        self.parent_id = int(row_id)

        # return leaf_uid
        leaf_uid = lineardb_name + ":" + str(row_id)

        # dump sqlite to binary
        db_url = os.path.join(cache_dir, "lineardb")
        with open(db_url, "rb") as f:
            lineardb_dump = f.read()
        lineardb_dump = codecs.encode(lineardb_dump, encoding="base64")

        # update lineardb on remote
        entrypoint_url = self.resource_uri + "/" + mongo_id + "/files"
        file_dict = {"lineardb": lineardb_dump}
        requests.post(entrypoint_url, files=file_dict)

        # delete cachedir
        os.remove(os.path.join(cache_dir, "lineardb"))
        os.rmdir(cache_dir)

        return leaf_uid

    def retrieve(self, uid: "LeafUID", length: int = 1) -> bytes:
        try:
            lineardb_name, row_id = uid.split(":")
            # set parent_id from retrieval
            self.parent_id = int(row_id)
        # check in case too many strings are returned
        except ValueError:
            raise ValueError("leaf_uid is not properly formatted.")

        mongo_id = _query_uid(self.resource_uri, lineardb_name)[0]["_id"]
        cache_dir = self._init_cache_dir()
        self._load_tree_into_cache_dir(cache_dir, mongo_id)

        db_url = os.path.join(cache_dir, "lineardb")

        bin = LinearBase.retrieve_trajectory(
            db_url=db_url, row_id=row_id, length=length
        )

        return bin

    def _init_cache_dir(self) -> str:
        return tempfile.mkdtemp()

    def _retrieve_tree_and_store_metadata(
        self, cache_dir: str, lineardb_name: str, data_bin: bytes
    ) -> str:
        """
        Retrieves existing LinearDB from resource_uri, or creating if doesn't
        exist.
        Also stores metadata when creating the LinearDB is required.

        Returns the ObjectID from ExpeDB.
        """

        response = _query_uid(self.resource_uri, lineardb_name)
        if len(response) == 0:
            # first-save logic below
            # make new chkpt and associate the uid
            mongo_id = _initialize_checkpoint(
                self.resource_uri, dict={"uid": lineardb_name}
            )

            # parse metadata
            loaded_obj = pickle.loads(data_bin)
            del loaded_obj.buffer
            metadata_bin = pickle.dumps(loaded_obj)

            # save metadata
            entrypoint_url = self.resource_uri + "/" + mongo_id + "/files"
            file_dict = {"metadata": metadata_bin}
            requests.post(entrypoint_url, files=file_dict)

        elif len(response) == 1:
            mongo_id = response[0]["_id"]
            # no need to resave metadata
            self._load_tree_into_cache_dir(cache_dir, mongo_id)
        else:
            raise Exception("ExpeDB is corrupted with duplicate checkpoints.")

        return mongo_id

    def _create_tree(cache_dir: str) -> None:
        """
        Create SQLite cache.
        """
        db_path = os.path.join(cache_dir, "lineardb")
        LinearBase.init_db(db_path)
        return

    def _load_tree_into_cache_dir(self, cache_dir: str, mongo_id: str) -> None:
        print(self.resource_uri + "/" + mongo_id + "/lineardb",file=sys.stderr)
        response_bin = requests.get(
            self.resource_uri + "/" + mongo_id + "/lineardb"
        )
        if response_bin.status_code != 200:
            raise Exception("Failed to retrieve lineardb from ExpeDB.")
        
        bin = codecs.decode(response_bin.content, encoding="base64")
        lineardb_path = os.path.join(cache_dir, "lineardb")
        with open(lineardb_path, "wb") as f:
            f.write(bin)
        return
